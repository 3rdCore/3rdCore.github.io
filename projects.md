---
layout: default
title: Projects
menu: main
permalink: /projects/
---

Projects
=========

Deep learning, generalization, and domain adaptation
------------------

<div id ="imagelist">
<ul>
      <li>
      <a href="https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update" target="_blank">
      <img src="{{ site.baseurl }}/images/project-bias-variance.png" alt="" title="A Modern Take on the Bias-Variance Tradeoff in Neural Networks">
      <div>
        A Modern Take on the Bias-Variance Tradeoff in Neural Networks
        <em>
	We measure neural network prediction bias and variance.
	We find that both bias and variance can decrease as the number of parameters
	grows. We decompose the total variance into
	variance due to training set sampling and variance due to initialization.
	</em>
      </div></a>
      </li> 
      <li>
      <a href="https://facebook.com/icml.imls/videos/live-from-icml-2019-in-long-beach-this-session-on-deep-generative-models-include/1269891676506524/"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-state-reification.png" alt="" title="State-Reification Networks">
      <div>
	     State-Reification Networks
        <em>
	     Improving Generalization
	by Modeling the Distribution of Hidden Representations
	</em>
      </div></a>
      </li> 
</ul>
</div>

Differentiable games
----- 

<div id="imagelist">
    <ul>
      <li>
      <a href="https://arxiv.org/pdf/1807.04740.pdf"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-negative-momentum.png" alt="" title="Negative Momentum for Improved Game Dynamics">
      <div>
        Negative Momentum for Improved Game Dynamics
        <em>
	Alternating updates are more stable than
	simultaneous updates on simple games.
	A negative momentum
	term achieves convergence in a difficult toy adversarial problem, but also on the notoriously
	difficult to train saturating GANs
	</em>
      </div></a>
      </li> 
    </ul>
</div>

Large-scale optimization and systems
----- 


Optimization and numerical analysis
-----
<div id="imagelist">
    <ul>
      <li>
      <a href="https://arxiv.org/abs/1707.02670" target="_blank">
      <img src="{{ site.baseurl }}/images/pca.png" alt="" title="Accelerated power iteration">
      <div>
	Accelerated stochastic power iteration
        <em>Exciting recent results on how adding a momentum term to the power iteration yields a numerically stabe, accelerated method.</em>
      </div></a>
      </li> 
    </ul>
</div>


Everything else
----- 

<div id="imagelist">
    <ul>
      <li>
      <a href="https://arxiv.org/abs/1707.02392"  target="_blank">
      <img src="{{ site.baseurl }}/images/shape-analogies.png" alt="" title="Representation and generation">
      <div>
	Representation Learning and Adversarial Generation of 3D Point Clouds
        <em>The first AutoEncoder design suited to 3D point cloud data beats state of the art in reconstruction accuracy. 
		GANs trainined in the AE's latent space generate realistic objects from every-day classes.
		</em>
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/pdf/1708.05256.pdf"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-15pf.png" alt="" title="Deep Learning at 15 Petaflops">
      <div>
        Deep Learning at 15 Petaflops
        <em>15-PetaFLOP Deep Learning system for solving scientific pattern classification problems on contemporary HPC architectures</em>
      </div></ak>
      </li> 
      <li>
      <a href="https://arxiv.org/pdf/1707.05807.pdf" target="_blank">
      <img src="{{ site.baseurl }}/images/graph.png" alt="Gibbs Sampling Scan Order" title="[redacted]">
      <div>
	Gibbs Sampling Scan Order
	<em>Our NIPS 2016 work shows that scan order matters.<br>
	Now we show that it is possible to make Gibbs sampling many orders of magnitude faster by customizing the scan sequence to a specific model and given set of target variables.</em>
      </div></a>
      </li> 
      <li>
      <a href="{{ site.baseurl}}/asynchrony/" target="_blank">
      <img src="{{ site.baseurl }}/images/theory-prediction.png" alt="Tuning parallel deep learning systems" title="Optimal momentum values for different levels of parallelization">
      <div>
	 Asynchrony and Momentum
        <em>When training large scale systems asynchronously, you get a momentum surprise.</em>
      </div></a>
      </li>
      <li>
      <a href="{{ site.baseurl}}/frogwild/">
      <img src="{{ site.baseurl }}/images/frog.png" alt="Picture of cute yet slightly deranged frog." title="Ribbit!">
      <div>
        FrogWild!
        <em>Fast PageRank approximations on Graph Engines</em>
      </div></a>
      </li> 
</ul>
</div>


Other
-------- 

<div id="imagelist">
    <ul>
      <li>
      <li>
      <a href="https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update" target="_blank">
      <img src="{{ site.baseurl }}/images/project-bias-variance.png" alt="" title="A Modern Take on the Bias-Variance Tradeoff in Neural Networks">
      <div>
        A Modern Take on the Bias-Variance Tradeoff in Neural Networks
        <em>
	We measure neural network prediction bias and variance.
	We find that both bias and variance can decrease as the number of parameters
	grows. We decompose the total variance into
	variance due to training set sampling and variance due to initialization.
	</em>
      </div></a>
      </li> 
      <li>
      <li>
      <a href="https://arxiv.org/pdf/1807.04740.pdf"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-negative-momentum.png" alt="" title="Negative Momentum for Improved Game Dynamics">
      <div>
        Negative Momentum for Improved Game Dynamics
        <em>
	Alternating updates are more stable than
	simultaneous updates on simple games.
	A negative momentum
	term achieves convergence in a difficult toy adversarial problem, but also on the notoriously
	difficult to train saturating GANs
	</em>
      </div></a>
      </li> 
      <li>
      <li>
      <a href="https://facebook.com/icml.imls/videos/live-from-icml-2019-in-long-beach-this-session-on-deep-generative-models-include/1269891676506524/"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-state-reification.png" alt="" title="State-Reification Networks">
      <div>
	     State-Reification Networks
        <em>
	     Improving Generalization
	by Modeling the Distribution of Hidden Representations
	</em>
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/abs/1707.02670" target="_blank">
      <img src="{{ site.baseurl }}/images/pca.png" alt="" title="Accelerated power iteration">
      <div>
	Accelerated stochastic power iteration
        <em>Exciting recent results on how adding a momentum term to the power iteration yields a numerically stabe, accelerated method.</em>
      </div></a>
      </li> 
      <li>
      <a href="http://dawn.cs.stanford.edu/2017/07/05/yellowfin/"  target="_blank">
      <img src="{{ site.baseurl }}/images/Yellowfin.png" alt="" title="Self-tuning!">
      <div>
	YellowFin: Self-tuning optimization for deep learning
        <em>Simple insights on the momentum update yield an very efficient parameter-free algorithm that performs well across networks and datasets without the need to tune any parameters.</em>
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/abs/1707.02392"  target="_blank">
      <img src="{{ site.baseurl }}/images/shape-analogies.png" alt="" title="Representation and generation">
      <div>
	Representation Learning and Adversarial Generation of 3D Point Clouds
        <em>The first AutoEncoder design suited to 3D point cloud data beats state of the art in reconstruction accuracy. 
		GANs trainined in the AE's latent space generate realistic objects from every-day classes.
		</em>
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/pdf/1708.05256.pdf"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-15pf.png" alt="" title="Deep Learning at 15 Petaflops">
      <div>
        Deep Learning at 15 Petaflops
        <em>15-PetaFLOP Deep Learning system for solving scientific pattern classification problems on contemporary HPC architectures</em>
      </div></ak>
      </li> 
      <li>
      <a href="https://arxiv.org/pdf/1707.05807.pdf" target="_blank">
      <img src="{{ site.baseurl }}/images/graph.png" alt="Gibbs Sampling Scan Order" title="[redacted]">
      <div>
	Gibbs Sampling Scan Order
	<em>Our NIPS 2016 work shows that scan order matters.<br>
	Now we show that it is possible to make Gibbs sampling many orders of magnitude faster by customizing the scan sequence to a specific model and given set of target variables.</em>
      </div></a>
      </li> 
      <li>
      <a href="{{ site.baseurl}}/asynchrony/" target="_blank">
      <img src="{{ site.baseurl }}/images/theory-prediction.png" alt="Tuning parallel deep learning systems" title="Optimal momentum values for different levels of parallelization">
      <div>
	 Asynchrony and Momentum
        <em>When training large scale systems asynchronously, you get a momentum surprise.</em>
      </div></a>
      </li>
      <li>
      <a href="{{ site.baseurl}}/frogwild/">
      <img src="{{ site.baseurl }}/images/frog.png" alt="Picture of cute yet slightly deranged frog." title="Ribbit!">
      <div>
        FrogWild!
        <em>Fast PageRank approximations on Graph Engines</em>
      </div></a>
      </li> 
