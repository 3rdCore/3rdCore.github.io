---
layout: default
title: ML class
menu: main
permalink: /ift6390-ml-class-2019/
---

IFT 6390: Fundamentals of machine learning
=========

An introductory but intensive graduate class in machine learning.

French title: Fondements de l'apprentissage machine

This is a graduate class taught in english. 
For a similar undergraduate class taught in french, please see 
Guillaume Rabusseau's <a href="https://www-labs.iro.umontreal.ca/~grabus/index.php?page=Teaching">
IFT3395
</a>.


**Requirements and prerequisites:**
The pace is meant to be intensive and the homework and exam requirements are going to be tough.
Before we begin, you should be reasonably knowledgeable in:
- Python programming
- Basic linear algebra
- Basic probability 

If not, you should be willing to work very hard at the beginning of the class to catch up with these requirements.
Otherwise you will find that this class is not a good fit for you.


People
------

Lecturer: [Ioannis Mitliagkas](https://mitliagkas.github.io), 

Teaching assistants: (TBA)

Communication
------------- 
We will use the class's [Studium page](https://studium.umontreal.ca/course/view.php?id=153979)
(not yet open for students) for communication.
The Studium page contains a forum for all discussions. 
This is a very large class, so the instructor will not be able to respond to individual questions about the class
via email. 
Make sure to use the class forum. 

**Exclusively in the event that you have a question of a sensitive, personal nature:**
Please feel free to email the instructor.
Please make sure to start the subject of your email with "[IFT6390]" to make sure that it is not lost. 



**Class times and rooms** see
<a href="https://admission.umontreal.ca/cours-et-horaires/cours/ift-6390/">
here</a>.

First class: Wednesday, September 4th 2019

Content and objectives of class
------------------
This course is an introduction to machine learning (ML), a field of research in artificial intelligence. The purpose of ML algorithms is to enable the machine to learn from examples. The focus of the course is classification and regression: we observe a set of pairs (observation, target) where the target is eg. a class and we try to predict the target corresponding to a new observation. The classic applications of these algorithms are in the field of pattern recognition (characters, image, speech, etc.). In recent years, several new fields of application have emerged: data mining, statistical analysis of textual data, quantitative management of financial portfolios, analysis of genetic expressions, etc. The subject of the course can be exploited in many fields of computer science.

The course will cover the following subjects: general notions (basic terminology, generalization, curse of dimensionality, capacity, classifier comparison), supervised algorithms (k-nearest neighbors, linear classifiers, neural networks, support vector machines, decision trees and regression, ensemble methods), unsupervised algorithms (principal component analysis, k-means method), overview of probabilistic graphical models.

Notions covered (indicative titles):
- Introduction and terminology of learning.
- Data Representation (+ Linear Algebra Reminders.)
-  The tasks of supervised learning (classification, regression) and unsupervised (density estimation, clustering, dimensionality reduction, ...)
-  Histogram-based methods. Curse of dimensionality. (+ reminders of probability and statistics)
-  Neighborhood-based methods (nearest neighbors and Parzen windows)
-  Multivariate Gaussian density
-  General framework of learning, performance evaluation, model selection, concept of capacity.
-  Principle of maximum likelihood v.s. Bayesian inference. Bayes classifier.
-  Regression and linear classification (logistic regression, perceptron, linear SVM, ...)
-  Classification and non-linear regression with kernel trick (+ optimization techniques)
-  Classification and non-linear regression: MLP-type neural networks (+ revision of calculation of partial derivatives)
-  More advanced neural network architectures: convolutional, recurrent.
-  Decision trees.
-  Aggregation methods: bagging and boosting
-  Unsupervised learning: dimensionality reduction (PCA, ...). Partitioning (k-means, ...)
-  Overview of probabilistic graphical models
-  Introduction to deep learning
-  Convolutional neural netoworks
-  Recurrent neural networks



<h2>Evaluation</h2>

*Please be advised that some details of this evaluation scheme might be adjusted slightly by the beginning of the semester.*


**Homework (20%):**
3 or 4 sets of homework including some theoretical exercises, programming and experimentation.
They are done in teams of 2 students.

**Class participation (5%):**
For actively participating in class and in the discussions on the class forum.

**Lab midterm exam (15%):**
This exam will take place in the lab and will test the programming abilities of the students

**Midterm exam(20%):**	
This exam will cover all the material seen until then. Date to be announced.

**Team project(20%):**
Application of learning algorithms to a student's field of interest and / or implementation of a new learning algorithm (programming, experimentation, and performance comparison). Several major themes (or areas of application of machine learning) will be proposed and teams will be able to form around these themes.
The project is done in teams.
The project must be approved by the instructor (details TBA).

**Final exam (20%):**
The whole class material. Precise date in December TBA.



Work and exams:
You can submit your work on the StudiUM site of the course. It is also on StudiUM that you will be able to see your grades for the homework and exams.

**ATTENTION regarding fraud and plagiarism:**
The University of Montreal now has a strict policy in case of fraud or plagiarism.
If an infraction is found, the professor is required to report to the director of the department.
An administrative procedure is then automatically triggered with the following consequences: the offense is noted in your file,
and a sanction is decided (which can be serious and go to dismissal in case of recidivism). It is important that you do the work yourself!


