---
layout: default
---

<div class="home">

<!--   <table style="text-align: left; width: 90%;" border="0" cellpadding="2" cellspacing="2"> --> 
	I am a Postdoctoral Scholar with the Departments of Computer Science and Statistics at Stanford University.
        I work 	with <a href="http://cs.stanford.edu/people/chrismre/">Chris R&eacute;</a> and <a href="http://web.stanford.edu/~lmackey/">Lester Mackey</a>.
	I got my PhD at The University of Texas at Austin with <a href="http://users.ece.utexas.edu/%7Ecmcaram/">Constantine Caramanis</a> and <a href="http://www.ece.utexas.edu/people/faculty/sriram-vishwanath">Sriram Vishwanath</a>,
	where I also worked with <a href="http://users.ece.utexas.edu/~dimakis/">Alex Dimakis</a>.
	<br>
	<br>
	I am currently on the academic job market:  <a href="cv.pdf">CV</a> and <a href="rs.pdf">research statement</a>. 
	<br>
	<br>

<table style="text-align: left;" border="0">
    <tbody>
      <tr>
        <td style="vertical-align: top;">


          <h2>Research Interests</h2>

          Machine Learning, High Dimensional Statistics, Large-scale Distributed Computation.
          <br><br>

	  Recent interests include <a href="{{ site.baseurl}}/asynchrony/">asynchronous optimization</a> and scan orders for Gibbs sampling.
          I have worked on resource-limited problems like memory-limited streaming PCA as well as streaming PCA with overwhelming erasures in the entries of each sample.
          <br><br>

          I also like breaking things in a way that makes them better/stronger/faster. FrogWild!, our work on PageRank approximation is a good example of this. Therein, working both atop of GraphLab and under the hood, we manage to get an improvement of 7x.
          <br><br>

          I have worked on most large scale computation platforms, from MPI to MapReduce and from GraphLab/Giraph to Spark. Lately, though, I have been looking at multi-core impementations with renewed interest.
          <br><br>

          Throughout our work I strive to bring together our very real implementation and the theory that will guarantee we -- most of the time! -- get a good result.
          <br><br>

          <br>
        </td>
        <td width=30px></td>
        <td style="width=30%;vertical-align: top;">
        <br>
        <br>
        <!--img title="Istanbul, 2013", style="max-width:200px;" alt="That's me." src="colourPortrait.jpg"></td-->
        <img title="Istanbul, 2013", style="width:100%;" alt="That's me." src="colourPortrait.jpg"></td>
        <!-- <img style="width: 200px; height: 300px;" alt="That's me." src="colourPortrait.jpg"></td>
         --><td style="vertical-align: top;">&nbsp;&nbsp; <br>
        </td>
      </tr>
    </tbody>
  </table>

<br><br>

<h2>Recent Projects</h2>
<div id="imagelist">
    <ul>
      <li>
      <a href="{{ site.baseurl}}/asynchrony/">
      <img src="{{ site.baseurl }}/images/theory-prediction.png" alt="Tuning parallel deep learning systems" title="Optimal momentum values for different levels of parallelization">
      <div>
	 Asynchrony and Momentum
        <em>When training large scale systems asynchronously, you get a momentum surprise.</em>
      </div></a>
      </li>
      <li>
      <a href="http://arxiv.org/pdf/1606.03432">
      <img src="{{ site.baseurl }}/images/scan-order.png" alt="Gibbs Sampling Scan Order" title="[redacted]">
      <div>
	Gibbs Sampling Scan Order
	<em>Does random scan mix at most O(logn) slower than systematic scan? NO!<br>
	Can we, in some cases, find an optimized scan order? YES!</em>
      </div></a>
      </li> 
      <li>
      <a href="{{ site.baseurl}}/">
      <img src="{{ site.baseurl }}/images/pca.png" alt="" title="Accelerated PCA">
      <div>
	Accelerated PCA [In preparation]
        <em>Exciting recent results on how adding a momentum term to the power iteration yields a numerically stabe, accelerated method. Stay tuned!</em>
      </div></a>
      </li> 
      <li>
      <a href="{{ site.baseurl}}/">
      <img src="{{ site.baseurl }}/images/momentum-dr.png" alt="" title="Self-tuning!">
      <div>
	Parameter-free optimization for deep learning [In preparation]
        <em>Simple insights on the momentum update yield an very efficient parameter-free algorithm that performs well across networks and datasets without the need to tune any parameters.</em>
      </div></a>
      </li> 
      <li>
      <a href="{{ site.baseurl}}/frogwild/">
      <img src="{{ site.baseurl }}/images/frog.png" alt="Picture of cute yet slightly deranged frog." title="Ribbit!">
      <div>
        FrogWild!
        <em>Fast PageRank approximations on Graph Engines</em>
      </div></a>
      </li> 
    </ul>
  </div> 
  <br><br>

  <h2>Recent News</h2>
  <ul>
		<li>January 2017: Visiting Microsoft Research, Cambridge
		<li>December 2016: In Barcelona for NIPS.
		<li>November 2016: Visiting Microsoft Research New England
		<li>November 2016: Full version of asynchrony <a href="papers/asynchrony-begets-momentum.pdf">paper</a>.
	  	<li>September 2016: Talk at Allerton
	  	<li>August 2016: I had the pleasure to give a talk MIT Lincoln Labs.
	  	<li>August 2016: Gave an <a href="{{ site.baseurl}}/asynchrony/">asynchronous optimization</a> talk at Google.
		<li>August 2016: <a href="http://stanford.edu/~imit/tuneyourmomentum/">Blog post</a> on our momentum work.
		<li>July 2016: <a href="https://arxiv.org/abs/1606.03432">Scan order paper</a> accepted at NIPS 2016!
	  	<li>July 2016: Invited to talk at NVIDIA.
		<li>June 2016: <a href="{{ site.baseurl}}/asynchrony/">Poster</a> at non-convex optimization ICML <a href="http://sites.google.com/site/noncvxicml16/">workshop</a>.
		<li>June 2016: <a href="https://arxiv.org/abs/1606.07365">Poster</a> at OptML 2016 workshop.
		<li>In a recent <a href="http://bit.ly/22wAt0e">note</a>,
		we show that asynchrony in SGD introduces momentum. 
		In the companion <a href="http://bit.ly/ovore">systems paper</a>, we use this
		theory to train deep networks faster.
		  <li>Our <a href="http://arxiv.org/pdf/1606.03432.pdf">paper</a> dispelling some common beliefs regarding the scan order
		 in Gibbs sampling.
		  <li>Does periodic model averaging always help? Recent <a href="https://arxiv.org/abs/1606.07365">results</a>.
		<li>Excited to start Postdoc at Stanford University. Will be working with Lester Mackey and Chris R&eacute;.</li>
		<li>Successfully defended my PhD thesis!</li>
		<li>SILO <a href="http://wid.wisc.edu/featured-events/silo032515/">seminar talk</a> at the Wisconsin Institute of Discovery. Loved both Madison and the WID!</li>
		<li>Densest k-Subgraph work <a href="http://devblogs.nvidia.com/parallelforall/gpu-accelerated-graph-analytics-python-numba/">picked up by NVIDIA</a>!</li>
		<li>Our latest <a href="papers/FrogWild.pdf">work</a> has been accepted for presentation at VLDB 2015!</li>
    </ul>
    <br>

  <h2>Publications</h2>
  <!-- {==% bibtex _plugins/style.bst biblio/biblio.bib %} -->

  {% include biblio.html %}

  <h2>Software</h2>
  Git <a href="http://github.com/mitliagkas" target="_blank">repositories</a> for the projects I'm working on.
  <br>
  <br>
  <br>



<!--   <h1 class="page-heading">Recent News</h1>

  <ul class="post-list">
    {% for post in site.posts %}
      <li>
        <span class="post-meta">{{ post.date | date: "%b %-d, %Y" }}</span>

        <h2>
          <a class="post-link" href="{{ post.url | prepend: site.baseurl }}">{{ post.title }}</a>
        </h2>
      </li>
    {% endfor %}
  </ul> -->

  <p class="rss-subscribe">subscribe <a href="{{ "/feed.xml" | prepend: site.baseurl }}">via RSS</a></p>

</div>
