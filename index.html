---
layout: default
---


<div class="home">

<!--   <table style="text-align: left; width: 90%;" border="0" cellpadding="2" cellspacing="2"> --> 

<table style="text-align: left;" border="0">
    <tbody>
      <tr>
        <td style="width=20%;vertical-align: top;">
        <img title="Diganta", style="width:100%;" alt="That's me." src="diganta.jpg">
	<br>
	<br>
        <img title="UdeM", style="width:50%;" alt="UdeM" src="images/logo-udem.png">
	<br>
	Incoming Graduate Student,
	<br>
	Computer Science,
	<br>
	University of Montr&eacute;al
	<br>
	<br>
	<a href="https://mila.quebec/" target="_blank">
        <img title="Mila", style="width:40%;" alt="Mila" src="images/logo-mila.png">
	<br>
	Incoming Graduate Student, 
	<br>
	Fall 2021
	<br>
	<br>
	</a>
	<a href="https://wandb.ai/site" target="_blank">
        <img title="Weights & Biases", style="width:90%;" alt="WandB" src="images/wandb1.png">
	<br>
	Machine Learning Engineer
	</a>
	<br>
	<br>
	<a href="https://landskapeai.github.io/" target="_blank">
        <img title="Landskape", style="width:70%;" alt="Landskape" src="images/lskape.png">
	<br>
	Founder & Researcher
	</a>
	<br>
	<br>
	<a href="https://www.lsr.hku.hk/member/diganta-misra/" target="_blank">
        <img title="HKU LSR", style="width:75%;" alt="LSR" src="images/hku.jpg">
	<br>
	Research Affiliate,
	<br>
	Laboratory for Space Research,
	<br>
	The University of Hong Kong
	</a>
	<br>
	<br>
	<br>

	<center>
	</center>
		<a href="cv.pdf" target="_blank">CV (January 2021)</a>
	<br>
	<br>


	<a href="mailto:mishradiganta91@gmail.com" target="_blank">Email,</a>
	<a href="https://scholar.google.com/citations?user=LwiJwNYAAAAJ&hl=en" target="_blank">Scholar,</a>
	<a href="https://twitter.com/DigantaMisra1" target="_blank">Twitter,</a>
	<a href="https://github.com/digantamisra98" target="_blank">GitHub</a>


	</td>
        <td width=30px></td>
        <td style="vertical-align: top;">
	I work on topics in optimization, dynamics and learning, 
	with a focus on modern machine learning. 
	I have done work in the intersection of systems and theory.
	<!--I am interested in systems for modern machine learning and data analysis.-->
	Some recent topics:
	<ul>
		<li>Min-max optimization and the dynamics of games</li>
		<li>Generalization and domain adaptation</li>
		<li>Optimization for deep learning</li> 
		<li>Statistical learning and inference</li>
	</ul>

	<center>
		See my <a href="rs.pdf" target="_blank">research statement</a>
		for long-term vision 
		and  <a href="https://scholar.google.com/citations?hl=en&user=K757SxgAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">recent publications</a>
		for an idea of what I'm doing now
	</center> 
    <br>
    I co-organize the 
    <a href="https://sgo-workshop.github.io/">Smooth games optimization and ML</a> 
    workshop series at NeurIPS.
    The <a href="https://slideslive.com/38924034/opening-remarks?ref=speaker-22001-latest ">opening remarks</a> video from last year gives a nice summary of our motivation for this line of work. 

    <br>
    <br>

    In Montreal, I co-organize <a href="https://mtl-mlopt.github.io/">MTL MLOpt</a>,
    a bi-weekly meeting of optimization experts from Mila, UdeM, McGill (CS and math), Google Brain,
    SAIL, FAIR, MSR, etc. 
    <br>
    <br>

	Every fall, I teach <a href="/ift6390-ml-class">ML</a> to 200 grad students.
	Every winter, I teach an advanced research class on <a href="/ift6085-dl-theory-class">deep learning theory</a>.

	<br>
	<br>



	Before joining the University of Montreal, I was a postdoc with the Departments of Computer Science and Statistics at Stanford University
        working with <a href="http://cs.stanford.edu/people/chrismre/" target="_blank">Chris R&eacute;</a> and <a href="http://web.stanford.edu/~lmackey/" target="_blank">Lester Mackey</a>.
	I got my PhD at The University of Texas at Austin with <a href="http://users.ece.utexas.edu/%7Ecmcaram/" target="_blank">Constantine Caramanis</a> and <a href="http://www.ece.utexas.edu/people/faculty/sriram-vishwanath" target="_blank">Sriram Vishwanath</a>,
	 where I also worked with <a href="http://users.ece.utexas.edu/~dimakis/" target="_blank">Alex Dimakis</a>.
	<br>
	<br>

	<font color="red" style="font-weight:bold">
		Prospective students: 
	</font>
	I am looking for particularly strong students, for MSc or PhD.
	Unfortunatley, I might not be able to respond to all emails.
	Please make sure to go over my  
	<a href="https://scholar.google.com/citations?hl=en&user=K757SxgAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">recent publications</a>
	and list of recent projects (below). 
	If you think that we have a strong overlap in interests and you have
<!--  already done research in the area feel free to email me. -->
	a strong background in mathematics and computation,
	please make sure to submit
	your <a href="https://mila.quebec/en/cours/admission/" target="_blank">supervision request</a> 
	 by December 15th (form opens in mid-October) and mention me as one of your faculty of choice.

	
	<br>


	</td>
        <!-- <img style="width: 200px; height: 300px;" alt="That's me." src="colourPortrait.jpg"></td>
         --><td style="vertical-align: top;">&nbsp;&nbsp; <br>
        </td>
      </tr>
    </tbody>
  </table>


<br>
<br>
<h2>Recent projects</h2>

<font color="red" style="font-weight:bold">
NEW: </font>
A curated list of some of my most exciting research projects. 
For a thorough list of projects grouped by topic, please consult the 
<a href="/projects">projects page</a>.
Publications listed below, in the present page.

<br>
<br>

<div id ="imagelist">
<ul>
      <li>
      <a href="https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update" target="_blank">
      <img src="{{ site.baseurl }}/images/project-bias-variance.png" alt="" title="A Modern Take on the Bias-Variance Tradeoff in Neural Networks">
      <div>
        A Modern Take on the Bias-Variance Tradeoff in Neural Networks
        <em>
	We measure prediction bias and variance in NNs.
	Both bias and variance decrease as the number of parameters
	grows. We decompose variance into
	variance due to sampling and variance due to initialization.
	</em>
	<br>
	Lead: Brady Neal
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/abs/1911.00804"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-adversarial-domain-generalization.png" alt="" title="Adversarial target-invariant representation learning for domain generalization">
      <div>
	Adversarial representation learning for domain generalization
        <em>
	We propose a process that enforces pair-wise domain invariance while training a feature extractor over a diverse set of domains.
	We show that this process ensures invariance to any distribution that can be expressed as a mixture of the training domains.
	</em>
	<br>
	Lead: Isabela Albuquerque, João Monteiro (INRS)
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/abs/2001.00602"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-games-spectral-shapes.png" alt="" title="Accelerating Smooth Games by Manipulating Spectral Shapes ">
      <div>
	Accelerating Smooth Games by Manipulating Spectral Shapes
        <em>
	We use matrix iteration theory to characterize acceleration in smooth games.
	The spectral shape of a family of games is the set containing all eigenvalues of the Jacobians of standard gradient dynamics in the family.
	</em>
	<br>
	Lead: Waiss Azizian
      </div></a>
      </li> 
      <li>
      <a href="http://papers.nips.cc/paper/8779-reducing-the-variance-in-online-optimization-by-transporting-past-gradients"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-igt.png" alt="" title="Reducing the variance in online optimization by transporting past gradients">
      <div>
        Reducing variance in online optimization by transporting past gradients
        <em>
        Implicit gradient transport turns past gradients into gradients evaluated at the current iterate.
        It reduces the variance in online optimization and can be used as a drop-in replacement for the gradient estimate in a number of well-understood methods such as heavy ball or Adam. 
    </em>
    <br>
    Lead: Sebastien Arnold
      </div></a>
      </li> 

    </ul>
</div>

  <br><br>

  <h2>Recent News</h2>
  <div id="" style="overflow-y:scroll; height:230px;">
  <ul>
		<li>January 2021: Manuela accepted at prestigious MSRI program for Fall 2021!
		<li>January 2021: Charles and Baptiste win best student paper award at OPT2020! For their paper on the <a href="https://arxiv.org/abs/2012.05782">fundamentals of condition numbers</a> in collaboration with Manuela. Their paper was accepted for publication at AISTATS 2021. 
		<li>January 2021: Alexia and Rémi's paper, in collaboration with MSR Montréal has been accepted at ICLR 2021! Preprint available <a href="https://arxiv.org/abs/2009.05475">here</a>.
		<li>September 2020: Paper on 
			<a href="https://arxiv.org/abs/2010.11924">evaluating generalization measures</a>
			accepted at NeurIPS'20.
		<li>September 2020: 
			Welcome to new PhD students, 
			<a href="https://ca.linkedin.com/in/ryan-d-orazio-05833979">Ryan D'Orazio</a> and 
			<a href="https://hiroki11x.github.io">Hiroki Naganuma</a>.
		<li>August 2020: Kartik Ahuja awarded the IVADO postdoctoral scholarship. Excited to have him join us in January 2021!
		<li>April 2020: Two papers on differentiable games 
			(<a href="https://arxiv.org/abs/2007.04202">one</a>, 
			<a href="https://arxiv.org/abs/1906.07300">two</a>)
			accepted at ICML'20.
		<li>January 2020: Two papers on efficient methods and tight bounds for differentiable games 
			(<a href="https://arxiv.org/pdf/2001.00602.pdf">one</a>, 
			<a href="https://arxiv.org/pdf/1906.05945.pdf">two</a>)
			accepted at AISTATS'20.
		<li>December 2019: Nicolas Loizou was awarded the IVADO postdoctoral scholarship at the prestigious Fellow tier.
		<li>December 2019: Brady Neal graduates with an MSc. He will continue on his PhD with us.
		<li>November 2019: Excited to be coorganizing the 2nd iteration of the <a href="https://sgo-workshop.github.io/">Smooth Games Optimization and Machine Learning Workshop</a> at NeurIPS'19.
		<li>November 2019: <a href="https://arxiv.org/abs/1906.03532">Reducing the variance in online optimization by transporting past gradients</a> selected for
		       	<span style="font-weight:bold">
			<font color="red">
				spotlight oral presentation	
			</font>
			</span> 
			at NeurIPS'19.
    </ul>
    </div>
    <br>

    <br>

  <h2>Publications</h2>
  <!-- {==% bibtex _plugins/style.bst biblio/biblio.bib %} -->
  {% include biblio.html %}

  <br>


  <h2>Past affiliations</h2>
        <img title="CSIR", style="width:15%;" alt="CSIR" src="images/csir.png">
        <img title="Paperspace", style="width:20%;" alt="Apogee" src="images/paperspace.png">
        <img title="IIT Kharagpur", style="width:20%;" alt="KGP" src="images/kgp.png">
	<br>
        <img title="AIESEC", style="width:18%;" alt="AIESEC" src="images/aiesec.png">
        <img title="Bangkok University", style="width:18%;" alt="BU" src="images/bu.png">
        <img title="Bennett University", style="width:20%;" alt="Bennett" src="images/bennett.png">
	<br>
		<img title="Leading India AI", style="width:18%;" alt="LeadingAI" src="images/leading.jpg">
        <img title="Digital Vidya", style="width:18%;" alt="DV" src="images/dv.jpg">
        <img title="Digimyx", style="width:20%;" alt="Digimyx" src="images/digimyx.png">
	<br>
		<img title="Coso", style="width:18%;" alt="Coso" src="images/coso.png">
        <img title="United Nations Volunteer", style="width:18%;" alt="UNV" src="images/unv.jpg">
		<img title="KIIT", style="width:20%;" alt="KIIT" src="images/kiit.jpg">
	<br>
	<br>
	

</div>
