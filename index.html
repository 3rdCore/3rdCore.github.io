---
layout: default
---

<div class="home">

<!--   <table style="text-align: left; width: 90%;" border="0" cellpadding="2" cellspacing="2"> --> 

<table style="text-align: left;" border="0">
    <tbody>
      <tr>
        <td style="width=20%;vertical-align: top;">
        <!--img title="Istanbul, 2013", style="max-width:200px;" alt="That's me." src="colourPortrait.jpg"></td-->
        <img title="Istanbul, 2013", style="width:100%;" alt="That's me." src="colourPortrait.jpg">

	<br>
	<br>

	<span style="font-weight:bold">
		Ioannis Mitliagkas
	</span>
	<br>
	Assistant Professor
	<br>
	Computer Science, University of Montr&eacute;al
	<br>
	<br>
	Core member of Mila
	<br>
	<br>
	Canada CIFAR AI Chair


	</td>
        <td width=30px></td>
        <td style="vertical-align: top;">
	I work on topics in optimization, dynamics and learning, 
	with a focus on modern machine learning. 
	I have done work in the intersection of systems and theory.
	<!--I am interested in systems for modern machine learning and data analysis.-->
	Some recent topics include:
	<ul>
		<li>Min-max optimization and the dynamics of games</li>
		<li>Optimization for deep learning</li> 
		<li>Statistical learning and inference</li>
		<li>Large-scale optimization</li>
	</ul>
	<font color="red">
		Teaching new version of the deep learning theory class in Winter 2019!
	</font>
		<br>

	      <a href="ift6085-dl-theory-class-2019/">
		IFT 6085: Theoretical principles for deep learning
		</a>
	<br>
	<br>

	Before joining UdeM, I was a Postdoctoral Scholar with the Departments of Computer Science and Statistics at Stanford University
        working with <a href="http://cs.stanford.edu/people/chrismre/">Chris R&eacute;</a> and <a href="http://web.stanford.edu/~lmackey/">Lester Mackey</a>.
	I got my PhD at The University of Texas at Austin with <a href="http://users.ece.utexas.edu/%7Ecmcaram/">Constantine Caramanis</a> and <a href="http://www.ece.utexas.edu/people/faculty/sriram-vishwanath">Sriram Vishwanath</a>,
	 where I also worked with <a href="http://users.ece.utexas.edu/~dimakis/">Alex Dimakis</a>.
	<br>
	<br>

	I receive many emails about available positions and unfortunately I cannot 
	respond to all of them. If you are interested in working with me, and have a
	strong background in mathematics and computation, please make sure to submit
	your <a href="https://mila.quebec/en/study/" target="_blank">application</a> 
	by December 15th. 
	<br>
	<br>


	<center>
	Contact:
	<a href="mailto:ioannis@iro.umontreal.ca" target="_blank">Email,</a>
	<a href="https://scholar.google.com/citations?user=K757SxgAAAAJ&hl=en&oi=ao" target="_blank">Scholar,</a>
	<a href="https://www.linkedin.com/in/ioannis-mitliagkas-6a8241131/" target="_blank">LinkedIn</a>
	<!--a href="http://github.com/mitliagkas" target="_blank">Github.</a-->
	</center>
	
	<br>


	</td>
        <!-- <img style="width: 200px; height: 300px;" alt="That's me." src="colourPortrait.jpg"></td>
         --><td style="vertical-align: top;">&nbsp;&nbsp; <br>
        </td>
      </tr>
    </tbody>
  </table>

<h2>Recent projects</h2>
<div id="imagelist">
    <ul>
      <li>
      <a href="http://dawn.cs.stanford.edu/2017/07/05/yellowfin/">
      <img src="{{ site.baseurl }}/images/Yellowfin.png" alt="" title="Self-tuning!">
      <div>
	YellowFin: Self-tuning optimization for deep learning
        <em>Simple insights on the momentum update yield an very efficient parameter-free algorithm that performs well across networks and datasets without the need to tune any parameters.</em>
      </div></a>
      </li> 
      <li>
      <a href="http://dawn.cs.stanford.edu/2017/07/05/yellowfin/">
      <img src="{{ site.baseurl }}/images/Yellowfin.png" alt="" title="Self-tuning!">
      <div>
	YellowFin: Self-tuning optimization for deep learning
        <em>Simple insights on the momentum update yield an very efficient parameter-free algorithm that performs well across networks and datasets without the need to tune any parameters.</em>
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/abs/1707.02670">
      <img src="{{ site.baseurl }}/images/pca.png" alt="" title="Accelerated power iteration">
      <div>
	Accelerated stochastic power iteration
        <em>Exciting recent results on how adding a momentum term to the power iteration yields a numerically stabe, accelerated method.</em>
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/abs/1707.02392">
      <img src="{{ site.baseurl }}/images/shape-analogies.png" alt="" title="Representation and generation">
      <div>
	Representation Learning and Adversarial Generation of 3D Point Clouds
        <em>The first AutoEncoder design suited to 3D point cloud data beats state of the art in reconstruction accuracy. 
		GANs trainined in the AE's latent space generate realistic objects from every-day classes.
		</em>
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/pdf/1707.05807.pdf">
      <img src="{{ site.baseurl }}/images/graph.png" alt="Gibbs Sampling Scan Order" title="[redacted]">
      <div>
	Gibbs Sampling Scan Order
	<em>Our NIPS 2016 work shows that scan order matters.<br>
	Now we show that it is possible to make Gibbs sampling many orders of magnitude faster by customizing the scan sequence to a specific model and given set of target variables.</em>
      </div></a>
      </li> 
      <li>
      <a href="{{ site.baseurl}}/asynchrony/">
      <img src="{{ site.baseurl }}/images/theory-prediction.png" alt="Tuning parallel deep learning systems" title="Optimal momentum values for different levels of parallelization">
      <div>
	 Asynchrony and Momentum
        <em>When training large scale systems asynchronously, you get a momentum surprise.</em>
      </div></a>
      </li>
      <li>
      <!--a href="{{ site.baseurl}}/frogwild/">
      <img src="{{ site.baseurl }}/images/frog.png" alt="Picture of cute yet slightly deranged frog." title="Ribbit!">
      <div>
        FrogWild!
        <em>Fast PageRank approximations on Graph Engines</em>
      </div></a-->
      </li> 
    </ul>
  </div> 
  <br><br>

  <h2>Recent News</h2>
  <div id="" style="overflow-y:scroll; height:200px;">
  <ul>
		<li>February 2018: <a href="https://arxiv.org/pdf/1706.03471.pdf">YellowFin</a> selected for
		       	<span style="font-weight:bold">
			<font color="red">
				oral presentation	
			</font>
			</span> 
			at SysML'18.
		<li>January 2018: Teaching new class! 
	      <a href="ift6085-dl-theory-class/">
		IFT 6085: Theoretical principles for deep learning
		</a>
		<li>December 2017: Accelerated power iteration via momentum, paper accepted at AISTATS 2018.
		<li>November 2017: Talk at Google Brain, Montr√©al
		<li>September 2017: Thrilled to be starting work at the University of Montreal and MILA as an assistant professor!
		<li>August 2017: Visiting my alma mater, UT Austin.
		<li>August 2017: At Sydney for ICML, presenting work on YellowFin, custom scans for Gibbs sampling, and deep learning for 3D point cloud representation and generation.
		<li>July 2017: 
			<span style="font-weight:bold">
			<font color="red">
				New preprint!
			</font>
			</span> Representation Learning and Adversarial Generation of 3D Point Clouds
				<a href="https://arxiv.org/abs/1707.02392">[arxiv]</a>.
		<li>July 2017: 
			<span style="font-weight:bold">
			<font color="red">
				New preprint!
			</font>
			</span> Accelerated stochastic power iteration
				<a href="https://arxiv.org/abs/1707.02670">[arxiv]</a>.
		<li>June 2017: 
			<span style="font-weight:bold">
			<font color="red">
				New preprint!
			</font>
			</span> An automatic tuner for they hyperparameters of momentum SGD 
				<a href="https://arxiv.org/pdf/1706.03471.pdf">[arxiv]</a>.
		<li>May 2017: Custom scan sequence paper accepted for presentation at ICML 2017!
		<li>April 2017: Invited talk at Workshop on Advances in Computing Architectures, Stanford SystemX
		<li>March 2017: 
		<span style="font-weight:bold">
		<font color="red">
			New preprint!
		</font>
		</span> Custom scan sequences for 
			<a href="https://arxiv.org/pdf/1707.05807.pdf">super fast Gibbs sampling</a>.
		<li>February 2017: Invited to talk at ITA in San Diego.
		<li>February 2017: Spoke at the AAAI 2017 Workshop on Distributed Machine Learning.
		<li>January 2017: Visiting Microsoft Research, Cambridge
		<li>December 2016: At NIPS, presenting our
			<a href="https://arxiv.org/abs/1606.03432">Gibbs sampling paper</a>
		  	dispelling some common beliefs regarding scan orders.
		<li>November 2016: Visiting Microsoft Research New England
		<li>November 2016: Invited talk at SystemX Stanford Alliance Fall Conference 
		<li>November 2016: Full version of asynchrony <a href="papers/asynchrony-begets-momentum.pdf">paper</a>.
	  	<li>September 2016: Talk at Allerton
	  	<li>August 2016: I had the pleasure to give a talk MIT Lincoln Labs.
	  	<li>August 2016: Gave an <a href="{{ site.baseurl}}/asynchrony/">asynchronous optimization</a> talk at Google.
		<li>August 2016: <a href="http://stanford.edu/~imit/tuneyourmomentum/">Blog post</a> on our momentum work.
	  	<li>July 2016: Invited to talk at NVIDIA.
		<li>June 2016: <a href="{{ site.baseurl}}/asynchrony/">Poster</a> at non-convex optimization ICML <a href="http://sites.google.com/site/noncvxicml16/">workshop</a>.
		<li>June 2016: <a href="https://arxiv.org/abs/1606.07365">Poster</a> at OptML 2016 workshop.
		<li>In a recent <a href="http://bit.ly/22wAt0e">note</a>,
		we show that asynchrony in SGD introduces momentum. 
		In the companion <a href="http://bit.ly/ovore">systems paper</a>, we use this
		theory to train deep networks faster.
		  <li>Does periodic model averaging always help? Recent <a href="https://arxiv.org/abs/1606.07365">results</a>.
		<li>Excited to start Postdoc at Stanford University. Will be working with Lester Mackey and Chris R&eacute;.</li>
		<li>Successfully defended my PhD thesis!</li>
		<li>SILO <a href="http://wid.wisc.edu/featured-events/silo032515/">seminar talk</a> at the Wisconsin Institute of Discovery. Loved both Madison and the WID!</li>
		<li>Densest k-Subgraph work <a href="http://devblogs.nvidia.com/parallelforall/gpu-accelerated-graph-analytics-python-numba/">picked up by NVIDIA</a>!</li>
		<li>Our latest <a href="papers/FrogWild.pdf">work</a> has been accepted for presentation at VLDB 2015!</li>
    </ul>
    </div>
    <br>

    <br>
  <h2>Publications</h2>
  <!-- {==% bibtex _plugins/style.bst biblio/biblio.bib %} -->
  {% include biblio.html %}

  <br>
  <br>

  <h2>Older Publications</h2>
  <!-- {==% bibtex _plugins/style.bst biblio/biblio.bib %} -->

  {% include biblio_older.html %}




<!--   <h1 class="page-heading">Recent News</h1>

  <ul class="post-list">
    {% for post in site.posts %}
      <li>
        <span class="post-meta">{{ post.date | date: "%b %-d, %Y" }}</span>

        <h2>
          <a class="post-link" href="{{ post.url | prepend: site.baseurl }}">{{ post.title }}</a>
        </h2>
      </li>
    {% endfor %}
  </ul> -->

  <p class="rss-subscribe">subscribe <a href="{{ "/feed.xml" | prepend: site.baseurl }}">via RSS</a></p>

</div>
